---
categories:
  - memo
date: 2024-10-18 00:00:00 +0900
math: true
tags:
  - paper
title: EMNLP2024 のタイトル眺めた
parse_block_html: true
published: true
---

進捗：現在まで Mixture-of-Subspaces in Low-Rank Adaptation まで追った。
これは人間がやる作業ではない。当日は似た論文でセッションが組まれるから、スケジュールを参考に追うのが良いんだけど、他分野との出会いを求めて全部見ちゃう。
できれば自動化したいけど AI/NLP のこと信用できなさそう。（多分信用してたら研究してない）

# 文埋め込み

## LongEmbed: Extending Embedding Models for Long Context Retrieval

Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li
https://arxiv.org/abs/2404.12096

文埋め込み界隈が好きそう。

> First, we examine the performance of current embedding models for long context retrieval on our newly constructed LongEmbed benchmark.

埋め込みモデルが長い文脈をどれだけ扱えるかを評価するベンチマーク。

> comprehensive experiments show that training-free context window extension strategies like position interpolation can effectively extend the context window of existing embedding models by several folds

学習無しでコンテキストウィンドウを拡張できることを（ちゃんと）確認した。

> our analysis reveals the superiority of RoPE-based embedding models over APE-based ones in context window extension. Hence, we advocate for the use of RoPE for future embedding models.

APE 終了宣言？

# 言語学的視点

## Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs

Kanishka Misra, Kyle Mahowald
https://arxiv.org/abs/2403.19827

芳賀さんぽさを感じる。

> Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question.

研究課題を要約の先頭に書いてくれるの助かる。
AANN は Article+Adjective+Numeral+Noun の略で、珍しい構文らしい（例：a beautiful five days）。
実験の結果、稀な構文はより稀でない構文から汎化によって学習できることが示唆された。

## A ∧ B ⇔ B ∧ A: Evaluating and Improving Logical Reasoning Ability of Large Language Models

Yuxuan WAN, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael Lyu
https://arxiv.org/abs/2401.00757

LogicAsker は、命題論理と述語論理に基づいた一連の原子的推論スキルを採用することで、このギャップに対処し、LLM の推論能力を体系的に調査・改善する。

## Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving

XIN QUAN, Marco Valentino, Louise A. Dennis, Andre Freitas
https://arxiv.org/abs/2405.01379

定理証明系と LLM を統合し、説明文を生成・形式化し、NLI の潜在的な推論戦略を提案する、Explanation-Refiner と名付けられたニューロシンボリックフレームワークを提案する。
上と似てる。

## CUTE: Measuring LLMs’ Understanding of Their Tokens

Lukas Edman, Helmut Schmid, Alexander Fraser

サブトークンで学習された LLM が本来の単語を知っているのかを評価した。

## Where is the signal in tokenization space?

Renato Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck
https://arxiv.org/abs/2408.08541

詳しく理解してないけど、tokenizer は [Tok,ens] と分割するけど model が [Tok,en,s] と出力するときの確率を分析する話。上と似た話題。

## Lexically Grounded Subword Segmentation

## A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners

Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J Su, Camillo Jose Taylor, Dan Roth
https://arxiv.org/abs/2406.11050

トークンの表層に影響を受けてちゃんと推論してないんじゃねという話。
Fig 2 は、[リンダ問題](https://ja.wikipedia.org/wiki/%E5%90%88%E6%8E%A5%E3%81%AE%E8%AA%A4%E8%AC%AC) というものがあるのだが、この例文のリンダをボブにして解けないなら、モデルはほんとうの意味でリンダ問題を理解してないのではという例みたい。

## DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing

Hangdi Xing, Changxu Cheng, Feiyu Gao, Zirui Shao, Zhi Yu, Jiajun Bu, Qi Zheng, Cong Yao

Document Hierarchy というフレーズが気になったが、論文が見つからなかった。

## Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification

Dongjun LIM, Yun-Gyung Cheong
https://openreview.net/forum?id=xXu4txKpBI

どうやったのかは詳しく読んでないけど、Expert が感情ごとに上手いこと分かれさせることで、expert gating score を見れば感情分析ができるってことかな？
MoE の説明で Expert が役割分担しているみたいな説明をたまに見るけど、実際はそんなに解釈しやすく学習されてないはず。でも、それをやってみようとしている点でこの論文は新鮮。

# Model/Architecture

## Rethinking the Reversal Curse of LLMs: a Prescription from Human Knowledge Reversal

Zhicong Lu, Li Jin, PeiguangLi, Yu Tian, Linhao Zhang, Sirui Wang, Guangluan Xu, Changyuan Tian, Xunliang Cai

論文見つからない。Reversal Curse よく聞くので気になってきた。

## Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons

Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng
https://arxiv.org/abs/2408.03247

推論中に事実を使っているのどうかを詳細に調べた。（言語推論に事実使わなくねと個人的には思うが批判的コメントは完読してから）

## Rethinking Token Reduction for State Space Models

Zheng Zhan, Yushu Wu, Zhenglun Kong, Changdi Yang, Yifan Gong, Xuan Shen, Xue Lin, Pu Zhao, Yanzhi Wang

State Space Model 気になるけど論文が見つからない。

## Backward Lens: Projecting Language Model Gradients into the Vocabulary Space

Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf
https://arxiv.org/abs/2402.12865

最近の解釈可能性手法は、フォワードパスから得られた重みと隠れ状態をモデルの語彙に射影し、LM の中で情報がどのように流れるかを明らかにする。本研究では、この手法を LM のバックワードパスと勾配に拡張する。
Integrated Gradient とは関係ない？

## Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps

Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James R. Glass
https://arxiv.org/abs/2407.07071

Attention を見ればハルシネーションを生成しようとしているか分かる。ほんとか？

# Generation

## RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models

> RAG introduces two major challenges.
> First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model's generation.
> Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers.

みんな気にする問題。タイトルは Medical となっているけど、あらゆる場面に関連する話。

## AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation

Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, Lidong Bing
https://arxiv.org/abs/2410.00558

よく分からないけど、GPT-4 の出力で蒸留するときにもっとうまくやろうって話？

# Training/Tuning method

## EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models

Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai
https://arxiv.org/abs/2402.09801

読んでないけど、ピンポイントで学習したことを解除できるの？

## Dissecting Fine-Tuning Unlearning in Large Language Models

Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang
https://arxiv.org/abs/2410.06606

上と同じ話題

## Exploring Reward Model Strength’s Impact on Language Models

Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen

Reward Model と Base Model のバランスは気になる。

## RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning

Haoyu Wang, Tianci Liu, Ruirui Li, Monica Xiao Cheng, Tuo Zhao, Jing Gao
https://arxiv.org/abs/2406.10777

LoRA で更新されるパラメータをスパースにすることで、FT 前後であまりパラメータを壊さないようにした。Fig. 1 を見れば一発でわかる。

## In-context Contrastive Learning for Event Causality Identification

梁超, Wei Xiang, Bang Wang
https://arxiv.org/abs/2405.10512

In-context Contrastive Learning ってなんすか。
Fig. 2 から察するに、ローカルモデルから取り出した出力埋め込みを使って対照学習するのかな？

## Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks

Haoyuan WU, Haisheng Zheng, Zhuolun He, Bei Yu
https://arxiv.org/abs/2401.02731

> Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity.

こんな話聞いたことなかった。そして、この問題の対処法が MoE 化というのも発想がぶっ飛んでる気がする。

# Position

## From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP

Marius Mosbach, Vagrant Gautam, Tomás Vergara-Browne, Dietrich Klakow, Mor Geva
https://arxiv.org/abs/2406.12618

調査回答や 556 本の論文の手動アノテーションの質的分析を通じて、NLP 研究者が IA 研究の知見を基礎とし、NLP の進歩にとって重要であると認識し、複数のサブフィールドで IA 研究の知見や用語を自身の研究に活用していることがわかった。

## Understanding "Democratization" in NLP and ML Research

Arjun Subramonian, Vagrant Gautam, Dietrich Klakow, Zeerak Talat
https://arxiv.org/abs/2406.11598

民主化という言葉を適当に使うなという話っぽい

<!-- ## Tokenization Is More Than Compression

Craig W Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner
https://arxiv.org/abs/2402.18376

ちょっときになる -->

<!-- ## Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences
Xiangyang Liu, Junliang He, Xipeng Qiu
https://openreview.net/forum?id=8t844SNkXx

経験 (Experiences) を溜めておいて、後で組み合わせる (Orchestrate) 枠組みを提案してるのかな？
ChatGPT に実装されてるパーソナライズ帰納と同じような気がするけど。 -->

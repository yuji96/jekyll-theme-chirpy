---
categories:
  - memo
date: 2024-07-10 00:00:00 +0900
math: true
tags:
  - ML
title: ICML2024 の気になる論文 (WIP)
parse_block_html: true
published: true
---

ICML accepted papers

{% linkpreview "https://openreview.net/group?id=ICML.cc/2024/Conference" %}

とりあえず oral だけ眺めた。oral のタブが 6 個なのに対して、poster のタブは 91 個もあるんだよね。全部追うのは無理では。

これ見る時間を自分用論文推薦システムづくりに当てたほうが良さそう（その方が楽しいだろうし）。今まで読んだ論文と傾向が近いものを拾う設計が多い浮かぶけど、出会いを失うのが怖い。

## 本記事の注意

- 論文の中身までは目を通してません。
- 説明は適当です。

## 感想

- position paper 多くね？あまり見たこと無いラインナップ。
- 思ってたより時系列予測が多い？

## oral

- [Improving Transformers with Dynamically Composable Multi-Head Attention](https://openreview.net/forum?id=RbiBKPtuHp)

  - attention head が独立してると冗長になることがあるから、動的に合成してる？
    - 冗長: 異なる head が似たような推論しちゃうこと
  - MHA が提案されてすぐに、異なるヘッドの推論を多様にしようぜみたいな話があった気がする。懐かしさ？を感じる（← 青二才がなんか言ってる）

- [Learning Useful Representations of Recurrent Neural Network Weight Matrices](https://openreview.net/forum?id=QBj7Uurdwf)

  - タイトルに惹かれた。一番いまの自分の研究に近そう。
  - 重みを直接分析する mechanistic アプローチと、入出力マッピングに着目する functionalist アプローチを対比する。順列等価な Deep Weight Space 層とプロービング入力を用いて、新しい functionalist アプローチは RNN の重みから有益な特徴を抽出する。言語生成と MNIST 分類のための新しいデータセットでの評価では、訓練されたタスクの予測において、 functionalist 手法が従来のアプローチを上回ることを示している。
  - RNN の分析をしやすくすることも目的の一つっぽい
  - 重みを直接分析する mechanistic approache を良いと思っていない？別に良い悪いとかないと思うが。

- [Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation](https://openreview.net/forum?id=uDkXoZMzBv)

  - Deep LoRA
  - 本アプローチは、オーバーパラメータ化された深い低ランク行列の復元に関する理論的知見に基づいており、各重み行列の学習ダイナミクスが不変な低次元部分空間に限定されることを示す。
  - これは、既存の低ランク適応（LoRA）手法を改善し、同等の効率を維持しながら、オーバーフィッティングの低減とハイパーパラメータの設定の簡素化をもたらす。
  - 特に、限られたデータで微調整を行う場合の自然言語タスクにおいて、Deep LoRA の有効性を検証する。

- [LoRA Training in the NTK Regime has No Spurious Local Minima](https://openreview.net/forum?id=s1sdx6vNsU)

  - LoRA の理論的理解？
  - LoRA を使うと偽の局所最適解にハマりにくくなる？

- [Arrows of Time for Large Language Models](https://openreview.net/forum?id=UpSe7ag34v)

  - 十分な大きさのモデルに対して、我々は経験的に自然言語を学習する能力に時間的な非対称性を発見した：次のトークンを予測しようとするときと、前のトークンを予測しようとするときの平均対数処理量の違いである。情報理論的な観点からは、このような違いはないはずである。

- [Interpreting and Improving Large Language Models in Arithmetic Calculation](https://openreview.net/forum?id=CfOtiepP8s)

  - 注目のヘッドの一部（<5%）が演算子やオペランドに集中し、これらの情報が多層パーセプトロン（MLPs）を通じて処理され、異なるデータセットやタスクでの計算能力を向上させる

- [DiJiang: Efficient Large Language Models through Compact Kernelization](https://openreview.net/forum?id=0uUHfhXdnH)
  - DiJiang は、離散コサイン変換（DCT）操作と加重準モンテカルロサンプリング法を使用して、事前学習済みのバニラ Transformer を線形計算量モデルに変換する

## みんな好きそう

- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://openreview.net/forum?id=6XH8R7YrSk)

  - 学術的なベンチマークで PPO と DPO を比較し、DPO が特定のシナリオで優れている一方で、LLM の微調整において PPO がより適応性があり効果的であることが分かった

- [Repoformer: Selective Retrieval for Repository-Level Code Completion](https://openreview.net/forum?id=moyG54Okrj)

  - X-former というタイトル久しぶりに見た
  - このフレームワークは、選択的な検索を使用することで効率性とロバスト性を改善し、最先端のコード補完性能を実現

- [Position: Do pretrained Transformers Learn In-Context by Gradient Descent?](https://openreview.net/forum?id=WsawczEqO6)

  - 理論的に ICL と GD は等価と言われている。このような接続は実際の事前訓練された言語モデルにおいて成り立つのか？

- [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://openreview.net/forum?id=dBqHGZPGZI)

  - DPO が有害な出力を効果的に避ける一方で、事前学習で獲得した能力自体は削除されず、むしろバイパスされる

- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://openreview.net/forum?id=3d5CIRG1n2)
  - DoRA の概要は、事前学習された重みを、微調整のために大きさ成分と方向成分に分解し、特に方向成分を効率的に更新するために LoRA を用いる。

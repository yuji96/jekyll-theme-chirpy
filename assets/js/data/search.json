[ { "title": "卒研の経緯", "url": "/posts/NLP2023-paper-history/", "categories": "article", "tags": "NLP, math", "date": "2023-04-06 00:00:00 +0900", "snippet": "NLP2023 で発表した『自己注意機構における注意の集中が相対位置に依存する仕組み (pdf)（以後、当研究）』は、いろいろと試行錯誤した末の着地点だけを述べていて、そういえばそこに至った経緯に触れていなかったので、卒研と学会を懐かしみながら書きました。（26 min Read は草。）自分の研究”だけ”に向き合っていると気づかなかったが、自分の分析方針はどうやら異質らしい。（自覚している範囲だと）下の 3 点が異質に感じる。 そもそも軸が違う 文章表現の周波数領域について分析している データではなくパラメータに対して特異値分解をしている etc.このそれぞれについて（etc. も含めて）4 セクションを分けて「なぜそうしたのか」を振り返る。そもそも軸が違う埋め込み表現の分野をざっくりと言い表すと、入力や出力やモデル内部から得られる埋め込み行列 $X$ の分析をしている。本記事では、この $X$ を下図のように定義する。 脱線 このブログの fork 元のデモを久しぶりに見たら、画像を回り込んで文章が書ける機能が追加されていた。反映したいけど merge しようとしたら conflict 大量発生して奇声あげそうなので保留…例えば、[&quot;I&quot;, &quot;will&quot;, &quot;be&quot;, &quot;back&quot;, &quot;!&quot;] というトークン列を埋め込み行列 $X$ で考えると、&quot;be&quot; の埋め込み表現は $X$ の 3 行目に対応する。一般的な研究では、単語に対応する埋め込みである各行について分析が行われている。しかし、当研究は位置に興味があったので、行分解して文をバラバラの単語にするのではなく列で分解した。これが一般的な方針と異なる理由だ。（正直、これはそこまで変なことをしているわけではないとは思う。）文章表現の周波数領域について分析しているこの見出しだけ見ると変態の所業に感じる。ただし、文章が時系列データだと抽象的に考えると、これも別にそこまで変態ではないはず。時系列データとは内容だけでなくその順番にも意味があるデータのことを指す。時系列データであるか否かは、データをシャッフルしていいかを考えると簡単に判定できる。例えば、音楽などはシャッフルしたらデータの意味が崩壊してしまう。では、文章はどうか。音楽のドレミと異なり、単語には重要度に差があるので、bag-of-words のように順番を無視しても推論できちゃったりするが、高精度を出すには単語の順序、つまり、文脈を捉える必要が出てくる。したがって、文章は時系列データの一種である。すると、音楽や音声などを扱う信号処理分野で日常的に行われるフーリエ変換を埋め込み行列 $X$ の列方向に適用するのも、変わっているかも知れないが荒唐無稽ではないだろう。（学会発表で振幅スペクトルの話の導入に、この辺のお気持ちを述べるべきだったと反省。）データではなくパラメータに対して特異値分解をしている特異値分解（SVD）は主成分分析（PCA）とほぼ同じようなもんらしいので1、見出しを「データではなくパラメータに対して PCA をしている」に変えてみる。すると、一気に変態な見出しになった。普通、 PCA といえば下の例のように、データ に対して行われる。 5 教科のテストの点数を PCA すると、理系っぽさと解釈できる主成分ベクトルが得られた。 多分、数学 + 理科 - 国語 - 社会 みたいな感じのベクトルになると思う。 word2vec の単語埋め込みを PCA すると、第 1 主成分が頻度、第 2 主成分が品詞とそれぞれ解釈できそうなベクトルが得られた。しかし、今回の対象はパラメータだから「？」となってしまう。このセクションの経緯を説明するのが一番難しいのでいくつかのサブセクションに分ける。初期目標: 中間層から位置埋め込み由来の成分を取り出すまず、最初はもちろん（パラメータではなくデータである）埋め込み行列 $X$ に対して PCA をして、位置表現のような主成分を得ようとした。他にも、音源分離に使われる独立成分分析（ICA）も試したりした（参考）23。このへんは ‘23 年度の夏休み前にしていたのであまり覚えていないが、別に PCA でも入力層付近ならそれっぽい成分を抽出することはできていた。ただ、あくまで「それっぽさ」までしか得られないので、胸張って「抽出できました！卒研終了！！」みたいな雰囲気ではなかった4。最終目標: 位置埋め込み由来の成分が文脈に従った推論に寄与することを示すおそらくこのもやもやした気持ちの原因は、PCA を使って初期目標を達成したとしても最終目標は 1mm も達成できないことに薄々気づいていたからだと思う5。仮に、PCA で位置埋め込みに由来する成分をほぼ完璧に抽出できたとする。しかし、その結果は PCA が文脈理解に必要な成分を抽出できることを示しただけで、文脈を捉える役目の自己注意機構がそれをできるかとは全く関係ない。静と動（このセクションを書いていて、自分ってこういうことに興味があったんだということに気づかされた。）この研究で示したいのは「埋め込みに〇〇という性質が存在する」という状態ではなく、「モデルが〇〇できる」という動作の方だった。すると、分析対象とすべきなのは埋め込み行列 $X$ ではなくパラメータの方ではないか。PCA は単純に入力 $X$ を直交変換して出力 $Y$ を計算する。（つまり、基底の取り替えにより視点を変えるだけで、データ $X$ を崩すようなことはしない。）すなわち、パラメータを直交行列 $U$ とおくと\\[Y = XU \\tag{a}\\]のようにとても単純に書ける。この解釈性の良さのおかげで PCA は重宝されている。あれなんかこの形見たことあるな。\\[Q = XW^Q,~K=XW^K \\tag{b}\\]クエリとキーですね。もしかして、PCA とか ICA とか面倒なことしなくてもクエリとキーをそのまま可視化すると実は面白い形をしてたりして…？6これがマジでした（位置に関して推論するために位置に関する成分を抽出する必要があるので納得はできる）。クエリとキーにバンドパスフィルタをかけてバイアスやノイズっぽいものを除くとよりきれいな形が見えてくる。このノイズはおそらく単語埋め込み由来の成分だと考えられる7。つまり、フィルタリング前は各次元に位置表現と単語表現が混在している。ならば、基底の取り替えれば位置表現だけが抽出できるのではないか。 表現が混在するとは NLP における埋め込み表現に関わらず、深層学習全般のニューラル表現分析でもこのような話は出てくる。 例えば、画像分類モデルのある層において、5 個のニューロンが 形, 色, 明るさ という 3 つの特徴を学習していたとする。 じゃあ、1 番目のニューロンが 形 を捉えていて、2 番目のニューロンが 色 を捉えていて、 なんてことはなく複数の特徴はいくつかのニューロンに散らばっている。 そこで、1 つのニューロンが 1 つの特徴を推論しているかのようにうまく表現を整理できれば、解釈性が大きく向上する。表現分析の目的は解釈性を向上させることなので、整理のためにエグい手法を使うことはおそらく少なく、分かりやすい単純な直交変換などが好まれる。特異値分解（SVD）の導入論文にも記載したが、クエリとキーの積 $QK^T$ を計算するときに $W^Q(W^K)^T$ を計算するため、パラメータ行列は 2 つあるように見えて実は 1 つの行列を分解したものである。もし、\\[\\begin{aligned} W^Q &amp;amp;\\leftarrow W^Q(W^K)^T \\\\ W^K &amp;amp;\\leftarrow I\\end{aligned}\\]のようにパラメータを置き換えても $QK^T$ は全く変わらない。つまり、クエリとキーはパラメータが混ざり合っていて、はっきりとした境界が存在しないため、お互いが独立した概念ではない。（例えば、クエリ $Q$ を作る重要な成分が $W^K$ にあり、その逆もあるかもしれない。）そのため、式 (a) と式 (b) は形は似ているが、$W^Q,~W^K$ 自体は特徴を抽出しているとは言えない。ここでの目標は\\[Q = XU^Q,~K=XU^K\\]における、PCA のパラメータ行列のような $U^Q,~U^K$ を得ることである。そして、PCA と同じく $U^Q,~U^K$ が直交行列として得られたらより嬉しい。ここで、SVD を用いると\\[W^Q(W^K)^T = U^Q S (U^K)^T\\]とパラメータが分解されて直交行列である左右特異ベクトルが得られる。この $U^Q,~U^K$ を $X$ に書けたものを可視化すると欲しかった表現が得られた。SVD でなぜうまくいくのかははっきりとは分からない。まあ、PCA や t-sne で表現が分かりやすくなることもあればそうでないこともあるのでそんなもんなのだろう。セクションまとめこのセクションのはじめに「データではなくパラメータに対して PCA をしている」という発言をしたが、これは自分のお気持ちとは全く異なる（数式ではそう見えちゃうけど）。やりたかったことは、一般的なケースと同じく、データ $X$ に対して基底変換をして解釈しやすい表現を得ることである。そのための変換行列 $U$ は §最終目標 でも触れた通り、パラメータ $W$ から得たかった。そこで $W$ から $U$ を得る過程で特異値分解を用いた。つまり、パラメータを基底変換することで、重要なパラメータの軸やその寄与度が欲しいというお気持ちはなく、普通に混じった表現を特徴ごとに分けたくて、その過程で SVD が有効だった。そもそも $W^Q(W^K)$ という混ざったパラメータが Transformer 以外でほぼ見かけないので、パラメータを整理するという発想も受け入れ難いのかもしれない。etc.クエリとキーは個別に考えられないと言ったそばから分解するのはなぜ一言でいうと、クエリとキーを個別に考えたかったから（考えられない ≠ 考えてはいけない）。クエリとキーの積を一つの塊として扱うと、ほぼほぼアテンション重み $A$ と同じ行列になってしまう8。自分の興味は「このヘッドでは位置を見てるね」「このヘッドでは係り受けを推論しているね」という結果を知ることではなく、「どうやって位置を見ているんだろう」「どうやって係り先を特定できるのだろう」という操作の方にある。この立場だと、アテンション重み $A$ はパラメータが色々操作した結果として得られた行列でしかない。なので、そうなった経緯を知るためには分解したくなる。 入力: 埋め込み行列 $X$ 途中経過: クエリ $Q$, キー $K$ （← ここが見たい） 観察結果: アテンション重み $A$埋め込み層だけでなく各層でも直に位置埋め込みを入れたほうがいい？残差接続（Residual Connection）があるので、最終層にも位置埋め込みを直で入力しているとみなせる（多分）。\\[\\begin{aligned} &amp;amp;\\xcancel{X = W + P }\\\\ &amp;amp;\\xcancel{layer_{12}( \\cdots (layer_2(layer_1(X) + X) + X) \\cdots ) + X}\\end{aligned}\\]訂正：こんなに単純な話ではない。1 層目の Attention の出力 $X$ と埋め込み層の出力 $W+P$ （単語埋め込み $W$ と位置埋め込み $P$ の和）に対する LayerNorm は、位置埋め込み $P$ とその他 $X+W$ の和に適用していると見れる。ここで、その他を無視すると\\[LayerNorm(* + P) = * + SPL\\]と書ける。ただし、$S$ と $L$ はスケールバラメータで対角行列。バイアスは $*$ に吸収させている。つまり、sub-layer を抜けるごとに位置埋め込みの両側の対角行列が増えていくので、$n+1$ 層目の入力は$layer_{n+1}(* + S_{n}^fS_{n}^a \\cdots S_{1}^fS_{1}^a \\cdot P \\cdot L_{1}^aL_{1}^f \\cdots L_{n}^aL_{n}^f )$みたいな感じになっている（のかな？）。これが正しければ、対角行列の積は対角行列だから結局は最初と同じ$layer_{n+1}(* + SPL )$と書ける。訂正前と大きく違うのは $S~(= \\text{diag}(\\text{Std}[token_i]))$ が左から掛かってしまうこと。絶対位置埋め込みが周期的なときに、左から行列がかかると、きれいな波形が がったがたになりそう。つまり、$S$ によっては位置埋め込みが壊れて上位層には伝わらなそう。（訂正部分終了）なので、モデルの分析をしたときに後層では位置埋め込み由来の成分が目立たない原因は、入力層から上手く伝わっていないのではなく、モデルが捨てていると思われる。「付き合ってください」って言われた数秒後に「”合” という漢字は 3 文字目だったな」と思うことはないだろう（言われたこと無いから知らんが）。このように後層に行くにつれて位置情報は徐々に不要になり、意味的推論に移行すると考えられる。編集後記自分なりの Self Attention の解釈 という記事は書き始めから 2 ヶ月経ってるのに1/3 くらいしか書いてないという怠慢なことをしているので、今回はそうならないように 1 日で一気に書ききった。脳内をかなりさらけ出したので恥ずかしさや怖さを感じている。 https://qiita.com/horiem/items/71380db4b659fb9307b4 &amp;#8617; $n$ 個の楽器からなるオーケストラを $d~(\\ge n)$ 個のマイクで収録すると $d$ 個の音源ができる。その音源に対して ICA を行うと、楽器ごとの $n$ 個の音源に分離できる（うまくいけば）。 &amp;#8617; 埋め込み表現だと、$n$ 通りの何らかの価値のある表現を $d$ 次元空間から上手く取り出すという風に言い換えられる。ただし、$n$ は未知なので 1~20 くらいの値で色々試すことになる。 &amp;#8617; 実際、PCA はそれっぽい軸を得るための手法なので、PCA を使う以上避けては通れない。 &amp;#8617; 当時はこの言語化できていなかった。 &amp;#8617; huggingface を使っている場合、$X$ は output_hidden_states=True とすれば取得できるが、$Q$ や $K$ はちょっとした実装をしないと得られないので、そのまま見たことある人は実はものすごく少ないのではないか。 &amp;#8617; どんな文章でもトークンの位置 [1, 2, 3, ...] はもちろん変わらない（当たり前過ぎて逆に変に聞こえるかも）。しかし、happy という単語は文によって 5 番目に現れたり 100 番目に現れたりするので、行列 $X$ の列方向に分析するときはほぼノイズにしか見えない。 &amp;#8617; scale や softmax の有無に違いはあるが、どちらも大小関係は変えないので、attention が当たるトークン関係は変わらない。 &amp;#8617; " }, { "title": "自分なりの Self Attention の解釈", "url": "/posts/attention-for-me/", "categories": "article", "tags": "NLP", "date": "2023-02-17 00:00:00 +0900", "snippet": "WIP ポエム せっかくルールが存在しない個人サイトなので、たまにはブログのような話から入ってみます。 卒論第一稿を書き終えて、久しぶりに何も無い日を 2 日くらい過ごせたので、結構メンタルが回復した気がします。しかし自分にはやらなければならないことがどうやら 2 つあるようです。それは呼吸器内科に行くことと歯科に行くことです。 さっき「あるようです」と語尾をぼかしたのは、単純にめんどくさいからです。家でボーッとしていたい怠惰な気持ち vs 病院に行かないと後悔する予感を対決させると怠惰が大差で勝利するので、いつも症状が深刻になってから病院に行きます。でもこの性格のおかげで散財する機会が減って、貯金が増えるという面もあるので悪いことばっかりってわけでもないです。体調は悪いですが。 &amp;lt;details markdown=&quot;1&quot; open&amp;gt; &amp;lt;summary&amp;gt;ただのメモ&amp;lt;/summary&amp;gt; `markdown=&quot;1&quot;` を指定すると、details 内でもコードブロックとか使えるのね。（数年前に使った記憶あるけど）\\&amp;lt;/details&amp;gt; なんか、自分に飽きたのでさっさと本題に入ります。さて、今回は卒研で 1 年弱付き合ってきた Transformer の Self Attention について書きます。解説というよりは、自分がどう Self Attention を解釈しているかという内容です。なので、ちょっとだけ Self Attention のことを知っているほうが面白いかもしれません。ちなみに、私は新米ゆえに、研究者が考えていることをどこまでオープンなところで喋って良いのか分からないので、当たり前なことだけ触れることにします（考えすぎだと思うけど）。分布仮説: 単語ベクトルの原点言葉を分散表現や埋め込み (Embedding) と呼ばれる数ベクトルで表そうという発想の根底には 分布仮説 (Distributional Hypothesis) という考え方があります。分布仮説とは、同じ文脈で出現する単語は似たような意味を持つ傾向があるという考え方で、ここから「単語は仲間によって特徴づけられる」という考え方に発展しました。次の例で実際に体験してみましょう。Q. [MASK] に入るのは何でしょう。 私は毎晩 [MASK] を見るのが好きです。 映画: 0.166 テレビ: 0.151 夢: 0.101 鏡: 0.052 空: 0.038 この [MASK] は最新型なので高価です。 モデル: 0.176 エンジン: 0.099 機種: 0.070 車: 0.053 カメラ: 0.028 [MASK] のリモコンを見つけられない。 リモコン: 0.181 テレビ: 0.093 ロボット: 0.034 自分: 0.026 カメラ: 0.023 私たちは、試合観戦をするために [MASK] を買った。 チケット: 0.200 馬: 0.071 車: 0.033 ユニフォーム: 0.019 テレビ: 0.017 皆さんは何を思い浮かべましたか。正解というわけではないですが、これらの文はいずれも テレビ のつもりで作成しました。ただ、1 個目だと 星空 の方がロマンチックで、4 個目は チケット の方がむしろ妥当ですね。 例文は ChatGPT に「テレビ という言葉で例文作って」と送ったときの返信です。 トグル ▶ を開くと表示される 5 個の単語は BERT の推論結果の上位 5 位です。Hugging Face で試すことができます。 とりあえず、[MASK] に当てはまる単語の選択肢が文脈により制限される、すなわち、「単語は仲間によって特徴づけられる」というのは共感してもらえたことにします。Self Attention における分布仮説まずは数式を使わずに Self Attention がやっていることを説明します。Self Attention は、内部で計算されるアテンション行列を可視化することで何をやっているのかを推測することができます。アテンション行列 (Clark, 2019) [CLS] と [SEP] はそれぞれ文頭と文末を示すトークンです。この記事では無視します。シンプルな左図だけ詳しく見てみましょう。このとき、Self Attention は以下の文が入力されたときに何を出力するか考えています。This market has been very badly damaged.そして、中間にある線は「左端の単語の意味が右端の単語に吸収される」様子と私は解釈しています。例えば、今回なら次のように線の右端の単語が情報を受け取って意味が更新されることを表しています。 情報 吸収先 気持ち This market 自分って名詞？ has been この文って現在完了？ very badly 強調あざす been damaged (1) 自分って受動態？ badly damaged (2) もしかして僕あまり良い意味の単語じゃない？ ここで、「吸収」とは単純に差し替えを意味しています。なので、プログラミング風に雑に書くならこんな感じになります。market = Thisdamaged = been + badlybeen = hasbadly = very割と誇張しましたが、Self Attention はこんな感じに単語ベクトルを更新しています。つまり、「単語は仲間によって特徴づけられる」というアイデアをとても直接的にモデル化しています。Self Attention の定義と解釈Self Attention は、クエリ $Q$ とキー $K$ とバリュー $V$ の 3 つを入力として、次のように定義されます。\\[\\begin{aligned} Q &amp;amp;= XW^Q, ~ K = XW^K, ~ V = XW^V \\\\ A &amp;amp;= \\text{softmax}\\left( \\dfrac{QK^T}{\\sqrt{d}} \\right) \\\\ \\text{Attention}(Q,K,V) &amp;amp;= AV\\end{aligned}\\]この $A$ がアテンション行列です。最後の式は、単語ベクトルからなるバリュー $V$ にアテンション $A$ を掛けることで、単語同士の重み付き和を計算しています。ただし、大体の重みをゼロと思えば、（大きな重みに対応する）関連語に自身を差し替えているとみなせます。\\[\\begin{aligned} x_2 &amp;amp;= a x_1 + by_1 + cz_1 \\\\ &amp;amp;= 0 \\cdot x_1 + 1\\cdot y_1 + 0\\cdot z_1 \\\\ &amp;amp;= y_1\\end{aligned}\\]「キーなんてなかった」よく、Self Attention は「クエリ・キー・バリュー」の 3 つで構成される辞書のようなものと言われます。去年は自分も辞書という例えで「なるほど」と思ったのですが、やはり、辞書のキーが変動するのがずっともやもやしていました。このせいで Transformer ベースの手法が数ヶ月理解できませんでした。じゃあ、キーなくせばいいやん！「クエリ・キー・バリュー」なんてものは所詮人が考えた概念です。（ノイジー・マイノリティ感のある発言。）なので、これらの概念を一旦忘れて数式を同値変形すれば自分なりの解釈をすることができます。私が解釈しやすいように書き換えると、こうなります。\\[\\begin{aligned} \\widetilde{Q} &amp;amp;= XW^Q(W^K)^T = X\\widetilde{W}^Q \\\\ \\widetilde{X} &amp;amp;= \\text{softmax}\\left( \\dfrac{\\widetilde{Q}X^T}{\\sqrt{d}} \\right) X\\\\ \\text{Attention}(Q,K,V) &amp;amp;= \\widetilde{X}W^V\\end{aligned}\\]最後の $\\text{Attention}(Q,K,V)$ の式に上 2 つの式を代入すると明らかですが、この定義は一般的な定義と全く等価です。" }, { "title": "M1 mac に labelImg を（きれいに）インストールする方法", "url": "/posts/labelImg/", "categories": "article", "tags": "labelImg, object-detection, mac", "date": "2022-09-20 00:00:00 +0900", "snippet": " こちらの記事と同じ内容です。https://blog.shinonome.io/shinkan-handson-data/現在、私は画像処理のプロジェクトに参加しています。この記事では、M1 mac に labelImg をインストールする手順を紹介します。M1 mac だと単純に pip でインストールしようと失敗してしまいますが、公式リポジトリに M1 mac をサポートしているリリース前バージョンがあるので、手元の環境をいじらずにインストールすることができます。labelImg とは画像データの調査やモデルを学習する際にアノテーションされたデータがたくさん欲しい場合があります。このとき、役に立つのがアノテーション専用のツールです。中でも、labelImg はインストールが簡単で、慣れてくると素早くアノテーションを作成することができます。発生した問題僕がもともと使っていた Macbook Air intel 2019 では、pip install labelImgを実行するだけでインストールすることができていました。GUI ソフトなのに手軽にインストールできるのが labelImg が愛される理由の一つだと思います。しかし、最近 M1 mac を買って環境を再現しようとしていた際に、エラーが発生してインストールできませんでした。Collecting labelImg Using cached labelImg-1.8.6.tar.gz (247 kB) Preparing metadata (setup.py) ... doneCollecting pyqt5 Using cached PyQt5-5.15.6.tar.gz (3.2 MB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... error error: subprocess-exited-with-errorこの原因は labelImg が依存している pyqt5 が M1 に対応していないためです。対応策labelImg では、GUI の実装する際に使用する外部ライブラリを pyqt5 ではなく pyside6 に移行する動きがあります。これが pyside6 ブランチで開発されていてすでに動く状態になっているようです。これを以下の手順でセットアップすると起動できます。 リポジトリをクローンする。最新の pyside6 ブランチのソースコードだけあればいいので shallow clone しています。 リポジトリに移動する。 labelImg の依存ライブラリである pyside6, lxml をインストールする。 make pyside6 でビルドする。 labelImg.py を実行する。git clone --depth=1 --branch pyside6 --single-branch https://github.com/tzutalin/labelImg.gitcd labelImgpip install pyside6 lxmlmake pyside6python labelImg.py一次ソース：https://github.com/tzutalin/labelImg/tree/pyside6#macos.zshrc にエイリアスを定義すると、labelImg をコマンドのように呼び出すことができます。この場合でもコマンドライン引数は使えます。alias labelImg=&#39;python path/to/labelImg/labelImg.py&#39;（ショートカットが使えなくなっているので、作業効率が悪くなりそうです。まだ開発中かもしれないので今後のアップデートに期待です。）" }, { "title": "macのセットアップ手順メモ", "url": "/posts/mac-migration/", "categories": "memo", "tags": "mac, vscode, zsh", "date": "2022-08-11 00:00:00 +0900", "snippet": "mac から mac へ移行した時の手順を記憶してる限り全部書いてみた。mac のシステム設定 カーソルの移動速度を速度を速くした キーのリピート速度を早くした メニューバーに充電残量を表示した キーボードの修飾キーで caps lock を esc にした spotlight 検索の対象をアプリのみに変更ブラウザ 元々使っていた Brave を同期させただけでブックマーク、拡張機能、パスワードを完全に移行できた。多分、Chrome にも似た機能はあると思う。shell の設定 先に brew, zinit, pyenv を入れておく 今まで使っていた zshrc をコピペして立ち上げるだけで再現できた。 gitconfig ファイルもコピペで移行する。 ssh key もコピペする。（本当は鍵作り直しの方が良いのかもしれないけど）zinit でインストールできるおすすめカスタマイズ powerlevel10k : かなりかっこよくなる zsh-users/zsh-completions : git とかの有名コマンドの補完をしてくれる zsh-users/zsh-autosuggestions : 過去に使用したコマンドを補完してくれる zdharma-continuum/fast-syntax-highlighting : ちょっとかっこよくなる zsh-users/zsh-history-substring-search : 過去に使用したコマンドの検索ができるvscode setting sync という標準機能で拡張機能、キーバインド、俺流 settings.json が一発で移行できた。感動。ただし、先に環境構築をしないと「python がありません」とかのエラーがいっぱい出てくるので注意。 code command を有効化する入力ソースを Google 日本語入力にするmac の日本語入力は変換候補が使っているうちにバ ○ になっていくので、Google 日本語入力に変更するのがおすすめ。 公式 HP インストール手順 入力ソースからデフォルトの英語を消す方法僕が M1 mac を買った当初はアプリによって動かなかったのにひどく落胆したが、気づいたらアプデされて使えるようになっていた。Google 大好き。アプリをインストールする zoom, line, slack, vscode, brave, office, deepl, etc.メール設定 メールアプリでログインするだけデータ移動ここから mac 移行の話。移行アシスタントは起動が遅く、全ファイル移そうとする。大抵の場合もういらないファイルが合ったりするので、新しい mac をハードディスクとして読み込んでコピったほうが良さげhttps://support.apple.com/ja-jp/guide/mac-help/mchlb37e8ca7/mac" }, { "title": "新歓ハンズオン資料編集後記", "url": "/posts/shinkan/", "categories": "article", "tags": "jupyter, dash, plotly", "date": "2022-05-08 00:00:00 +0900", "snippet": " こちらの記事と同じ内容です。https://blog.shinonome.io/shinkan-handson-data/この記事では、新歓ハンズオン（オンライン開催・オフライン開催）のデータコースで作成した資料や当日についての振り返りをしていきます。資料全体の流れ簡単に言うと、手書きのアルファベットの画像を読み込んで何が書かれているのかを人工知能に当てさせるということをしました。 EMNIST データセットを読み込む。 多層パーセプトロン（ニューラルネットワークの一種、以下 MLP）を構築する。 MLP を学習する。 混同行列を作成してエラー分析をする。 実際に自分が書いたアルファベットを MLP に予測させる。資料は GitHub で公開しています。詳細はこちらをご覧ください。https://github.com/yuji96/data-science-notes/blob/main/%E6%96%B0%E6%AD%93hands-on/notebook_with_output.ipynb推し機能canvas on Jupyterふと、「Jupyter 　上に canvas を表示して自由に書いた画像をモデルに予測させることってできるかな？」と思いました。Python でグラフ描画といえば matplotlib と思われがちですが、最近 Plotly というグラフをインタラクティブに操作できるライブラリが伸びてきています。その Plotly のグラフを Web アプリに組み込むための Dash というサービスがあります。（正直、規模が大きくなると　 Python のコードが汚くなるので Web 開発は TypeScript の方がやっぱり向いてるなと感じます。）どうやら jupyter_dash を使うと Jupyter 上で Dash を起動できるようです。さらに、dash-canvas を使うと Dash 上に canvas を表示できるようです。Q. あれ、これって組み合わせれば Jupyter 上に canvas 表示できるのでは？A. できちゃった ♪実はこの機能はハンズオン資料のために作ったわけではなく、この機能が完成した達成感の余韻で気づいたらハンズオン資料ができていたというのが正しい経緯です。混同行列がちょっと面白い難しい話になるので詳細は割愛しますが、良いモデルの混同行列は対角成分の値が大きくなります。でもこの画像では対角行列から 26 文字分ずれたところにも薄っすらと水色の線が見えます。これは人工知能が w を W を間違えたり c を C と間違えたりしているということを意味しています。ちょっと人間らしくて面白かったです。工夫したことGoogle ColaboratoryGoogle Colaboratory は便利すぎて、今では当たり前のツールになってきています。プログラミングをしたことがない初心者がターゲットということもあり、環境構築が不要なサービスにする必要があったため、ブラウザ上で GPU まで使える Google Colaboratory 一択でした。GPU 使ってなかったら早めにエラーを出すパラメータが数万規模の MLP の学習には GPU が必須になりますが、気づかずに CPU のまま進めてしまうと学習にとてつもない時間がかかることになります。普段だったら「やっちまった～、最初からやるか」で済みますが、今回のハンズオンは 30 分の時間制限があったので早い段階で気づいてあげる必要があります。そこでまず最初に以下のコードを実行することで、全員 GPU が使用されていることを確認しました。assert tf.config.experimental.list_physical_devices(&#39;GPU&#39;)バッチサイズを大きめにしたバッチサイズとはモデルを学習する際に指定する値で、大きくすると計算の並列性が上がることで学習がはやくなり、小さくするとモデルが局所最適解にはまりにくくなるので汎化性能の向上が見込めます。今回は時間の制約があり精度よりも速度の方が重要だったので大きめにしました。ライブラリを GitHub のハッシュ値を指定してインストールしたdash-canvas という Jupyter NoteBook 上にキャンバスを表示してお絵描きできるライブラリを使用したのですが、キャンバスサイズを指定しても効かないというバグがありました。別ブランチで修正されていたのですが未リリースだったため pypi からではなく GitHub から直接修正版をインストールしました。# Beforepip install dash-canvas# Afterpip install git+https://github.com/plotly/dash-canvas.git@df6e16db3ee56e93674faff6b1d2dd28ef4b3094この資料をきっかけに PlayGround に興味を持ってくれた人がいたら嬉しいです。" }, { "title": "分野横断すると見えてくるもの", "url": "/posts/cross-field/", "categories": "article", "tags": "science", "date": "2022-03-13 00:00:00 +0900", "snippet": " こちらの記事と同じ内容です。https://blog.shinonome.io/interpreting-formulas/今回は、統計や機械学習の分野でよく使われる３つの数式 平均 平均二乗誤差関数（RMSE） 正規方程式を僕はどのようにイメージしているかを紹介します。理解に行き詰まってしまったら理論的な部分は飛ばしつつ図や例えに沿ってぜひ最後まで読んでいただけたら幸いです。平均データが手元にあるときは、データの合計を個数で割るという公式が使われます。（標本平均）\\[\\frac{1}{N} \\sum_{i=1}^N x_i = \\frac{x_1 + x_2 + \\cdots x_N}{N}\\]可能かどうかは置いといて、仮に世の中にあるすべてのデータを手に入れたときにそれらの平均を求めたいときは、下式を使います。（期待値）（この解釈があっているのかは分かりません。）\\[\\text{E} [X] = \\int x p(x) ~\\text{d}x\\]積分を使うことで $x$ が無限通りの値を取る場合も考慮することができます。さて、ここからが本題です。僕は１年生で履修した物理の授業でこのような公式に出会いました。\\[G =\\int xm(x) ~\\text{d}x\\]これは物体の重心の座標を求めるための公式です。（簡単のために、物体全体の質量は１としています。）参考: https://science-log.com/数学/図形の重心を解析的に求める方法/記号が違うだけで完全に同じ形ですよね？物理において重心とは重力の釣り合いが取れる点、つまり指一本で支えられる点を意味します。本を指一本で支えるには真ん中に置けばいいですが、金槌のように位置によって密度が異なる物体を支えるには、重い方にずらさなければなりません。式の形が同じということは、確率分布に対しても同様に解釈しても良いはずです。このように、僕は「平均」のことを「真ん中」ではなく、分布を下から支えられる位置というイメージを持っています。平均二乗誤差関数（RMSE）下式で表される平均二乗誤差関数は最もよく使われるな誤差関数です。\\[\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - t_i)^2}\\]ここで、$y_i$ は モデルの予測で、$t_i$ は観測値とします。すると、$(y_i - t_i)$ は差なので予測の外れ具合と解釈できます。では、二乗して平均をとってルートを取ることにはどんな意味があるのでしょう。多くの人が気づいているかもしれませんが、定数倍すればこの式の形は（ユークリッド）距離と一致します。ユークリッド距離とは、点 P と点 Q の距離を P と Q を結んだ線の長さであると定義した最も直感的な距離です。参考: https://ja.wikipedia.org/wiki/ユークリッド距離#N次元観測値をヒトの力で変えることはできませんが、モデルのアルゴリズムを変えれば予測値は変えることができます。つまり、誤差を減らすことでモデルを改善することは、固定された点 Q に点 P を動かして近づけることと等価です。正規方程式先程の話で点 P を動かして点 Q に近づけるという話が出てきましたが、どこまで近づけられるのでしょうか。それは正規方程式から考えることができます。\\[\\hat{w} = (X^T X)^{-1} X^T t\\]正規方程式とは、$y = Xw$ で定められたモデルの最適なパラメータ $\\hat{w}$ を求めるときに使う公式です。ここで $w$ に $\\hat{w}$ を代入すると、\\[\\begin{aligned} y = X\\hat{w} = X(X^T X)^{-1} X^T t = Pt \\\\ (P = X(X^T X)^{-1} X^T)\\end{aligned}\\]$P$ という射影行列と呼ばれる形が現れます。と言われても、大学でもあまり見かけない式です。参考: https://manabitimes.jp/math/2486名前の通り射影 $P$ とは、$t$ という矢印を $X$ というスクリーンに写すことを言い、スクリーンに写った影に当たるのが左辺の $y$ です。よく射影に $P$ や $p$ という記号が使われるのは、projection の頭文字から来ています。（写像が mapping だから 射影写像はプロジェクション・マッピングだったりして。）射影とは、いくつかの次元を消す操作です。例えば、三次元の世界を写真に収めたら奥行きという次元を消した二次元の影ができます。しかし、写真は制限された空間でそれ以上のものを表現しようとしています。ではなぜ、パラメータ最適化の話に射影がでてくるのでしょうか。それは、モデルも同様に制限された空間からなんとか現実を表現しようとしているからです。例えば、\\[y = ax^2 + bx + c\\]というモデルは 2 次関数までしか表現できないという制限があります。つまり、パラメータの最適化とは、限られた領域（下図では $X$ という矢印の上）でしか動けないモデル $y$ を固定された $t$ に近づけることを意味します。その近さを測るが先程の RMSE のような誤差関数です。そして、モデル $y$ が $t$ の射影になったとき、つまり影になったときに誤差関数は最小になり、最適なモデルが完成します。射影をすると次元が減るので情報が失われてしまうのではないかと思われるかもしれません。マスコットの写真を取るとき、奥行きが失われるので一番カメラに近い表面部分しか写真に残すことができません。でも、中身のオジ ○ ンを撮りたい訳ではないのでそれで十分なのです。とても冗長な現実世界のデータからいらない情報を削ぎ落として欲しい情報だけを取り出すことが機械学習の目的なのです。正射影についての補足行列 $X$ をベクトル $x$ に書き直して -1 乗を分数にしたら、高校の教科書にも載っている正射影の公式になります。\\[y = \\frac{x^T t}{x^T x}x\\]" }, { "title": "Python でファイルを相対的に読み込む方法", "url": "/posts/relative-file/", "categories": "article", "tags": "python", "date": "2022-02-07 00:00:00 +0900", "snippet": " こちらの記事と同じ内容です。https://blog.shinonome.io/python-importfile/こんにちは。今まで、C++, JavaScript, R などの言語に触れたことはありますが、結局 Python に返ってきてしまう Yuji です。やりたいこと今回の目標は、以下のような階層構造があったときに、reader.py から data.csv を読み込むというとてもシンプルなことです。~/workspace/tmp_dir ├── data.csv └── reader.py（簡単そうでしょ？本当かな？）普通にファイル名を指定した場合では、そのままファイル名を指定してみると、import pandas as pdtry: pd.read_csv(&quot;data.csv&quot;) print(&quot;成功&quot;)except FileNotFoundError: print(&quot;失敗&quot;)&amp;gt; python reader.py成功もちろん問題なく成功しました。しかし少し意地悪をすると、&amp;gt; cd ..&amp;gt; python tmp_dir/reader.py失敗ファイルが見つからないと言われてしまいました。原因コマンドを見ると明らかですが、cd .. で一つ上の階層に移動したことで Python を実行した階層から見たファイルの位置が data.csv から tmp_dir/data.csv に変わったことが原因です。なので、reader.py の4行目を pd.read_csv(&quot;tmp_dir/data.csv&quot;)と変更すれば（一応）解決します。しかし、Python を実行する階層は人によって変わったり、他のファイルに import されたりするかもしれないので、その度にコードを書き換えると GitHub 上で戦争が起きたりローカルの差分が荒れたりします。絶対パスも人によって異なるので解決策とは言えないでしょう。ちなみに、Jupyter では階層を移動するという概念がないので、ファイル名の直指定でうまくいきます。（Colab ではあるみたいだけど。）解決策__file__ と pathlib（または、os.path ）を使いましょう。Python のコードを読んだり書いているときに __file__ という変数を見たことはあるでしょうか。試しに reader.py をこれだけにして実行してみましょう。print(__file__)&amp;gt; python reader.pyreader.py&amp;gt; cd ..&amp;gt; python tmp_dir/reader.pytmp_dir/reader.pyPython を実行した階層から見たファイル名が出力されました。では、次はこのようにすると、from pathlib import Pathprint(Path(__file__).resolve().parent)&amp;gt; python reader.py~/workspace/tmp_dir&amp;gt; cd ..&amp;gt; python tmp_dir/reader.py~/workspace/tmp_dirどこから実行しても同じ出力が得られました。resolve は相対パスを絶対パスに変換するメソッドで、parent は親ディレクトリを返します。つまり今回は、reader.py がある tmp_dir の絶対パスを取得できています。最後に、これらを使って今回の目標だった data.csv を読み込んでみましょう。from pathlib import Pathimport pandas as pdtry: parent = Path(__file__).resolve().parent pd.read_csv(parent.joinpath(&quot;data.csv&quot;)) print(&quot;成功&quot;)except FileNotFoundError: print(&quot;失敗&quot;)&amp;gt; python reader.py成功&amp;gt; cd ..&amp;gt; python tmp_dir/reader.py成功これでどこから実行してもファイルを読み込むことが出来ました！このような小技は適用範囲が広いので恩恵は大きいです。「なんか今のやり方スッキリしないなー」という小さな煩わしさを妥協しないようにしてると小技も少しずつ身についていくと思います。" }, { "title": "flatten vs ravel vs reshape（1次元化）", "url": "/posts/to-one-dim/", "categories": "memo", "tags": "python, numpy", "date": "2022-02-07 00:00:00 +0900", "snippet": "参考: https://note.nkmk.me/python-numpy-ravel-flatten/flatten vs ravel vs reshape（1 次元化） method 速度 返り値 flatten 遅い コピー reshape(-1) 早い （出来る限り）ビュー ravel 早い （出来る限り）ビュー ravel より reshape(-1) の方がビューが返りやすい。サイズが大きい配列を使用している場合は reshape(-1) を使った方が良いのかもしれない。あまり、1 次元化した後に元の配列をもう 1 回使いたいケースに今まで出会ったことは無い気がするので、わざわざ遅い flatten は使わなくて良いのかなーと思った。" }, { "title": "いい加減 Jupyter で仮想環境使うコマンド覚えろ俺", "url": "/posts/venv-kernel-to-jupyter/", "categories": "memo", "tags": "jupyter, ipython, venv", "date": "2021-12-26 00:47:00 +0900", "snippet": "pip install ipykernel pip-autoremovepython -m ipykernel install --name &amp;lt;name&amp;gt;jupyter kernelspec listpip-autoremove ipykernel -y" }, { "title": "初心者がPythonのインストールでやりがちなこと", "url": "/posts/pyenv/", "categories": "article", "tags": "python, for-beginner", "date": "2021-12-18 19:34:00 +0900", "snippet": "仮想環境という概念を知ろう 前提: brew がインストール済みの MacOSちょっとしたエピソード身の回りでこんな現象をよく聞く。 登場人物 A さん：プログラミングを始めたてで日々精進している。 B さん：大学の授業以外ではプログラミングは一切しない。 C 先生：ごく一般的な大学の先生。 A さん「プログラミング独学始めるぞ！公式サイトに行って Python をインストーーール！」A さん 「これどうやってやるんだろ。（検索中）。なるほど、pip install (...) か！できた！」〜〜数日後、授業でもプログラミングがはじまる〜〜C 先生「Anaconda をインストールして下さい。」A さん「はーい。」B さん「はーい。」〜〜外部ライブラリを使っていると…〜〜A さん「あれ、外部ライブラリが使えない。なんで！？」B さん「俺、普通に使えるけど？」熱心にプログラミングの勉強をしている人ほど PC 環境が荒れていく…これが起きてしまう理由原因これは、A さんの PC 内に Python（のインタープリタ）が、独学用と授業用の 2 つ存在してしまっていることが原因の可能性が高い。ターミナルから Python ファイルを実行するときは python a.py というコマンドを打つが、これは /usr/bin/python a.py や anaconda3/bin/python a.py とかの略である。python がどっちを指定しているかは、環境変数というもので定義されている。ただ、これをいじるのは面倒だし難しいし危険なこともある。背景この問題が起きるのは正直仕方ない。外部ライブラリを管理するツールである pip を使う場合、外部ライブラリの管理を自力でやらないといけない。なので、もし授業で使うとなると先生がいちいち指示しなければならなくなる。そこで、Anaconda というソフトは大体の人が使いそうなライブラリが同封されているので、先生は「このソフト入れといて」と指示するだけで済む。参考書の 0 章で Anaconda が紹介されている理由もこれだろう。ただ、インターネット上では、どちらかというと pip の方が人気なので、気づかずに環境を衝突させがちになる。解決策: python は pyenv で入れようbrew install pyenvpyenv install 3.10.0 &amp;lt;- お好みのバージョンpyenv global 3.10.0 &amp;lt;- お好みのバージョンこれで python 3.10.0 がインストールされる。とりあえず最新版を入れておけば問題無いが、Python に限らず新しすぎると他のツールが追いついてなかったりネット上の情報が積もってなかったりするので、2 個くらい下のバージョンの方が安定する。もしやらかしたらwhich pythonpython --versionpyenv versionsとかを見れば自分がどの Python を使っているのかが分かる。とりあえずこれを実行したときのスクショを先輩に見せれば良い感じのアドバイスがもらえるかもしれない。ライブラリレベルの仮想環境は venv で入れようと思ったけど、はじめのうちは混乱の元になるからやらなくて良いと思う。 プロジェクト毎に環境を分けると PC のストレージを割と圧迫するっぽい。 複数プロジェクトを並行でやっていると混乱する。 tensorflow みたいに、バージョンがちょっと変わると挙動がめっちゃ変わるライブラリの場合は、仮想環境が必要。 Pipenv とかの他のツールもあるけど、個人的に venv で十分。デプロイ先が Pipenv に対応してないから venv で作り直すこととかもあったし。" }, { "title": "初めてOSS開発（IPython）にコミットした話 ②", "url": "/posts/first-OSS-2/", "categories": "blog", "tags": "ipython, sphinx, github", "date": "2021-09-27 01:42:00 +0900", "snippet": "この記事は、初めて OSS 開発（IPython）にコミットした話（その１）の続きです。いざやってみるとりあえず CONTRIBUTING.md を読む規模の大きいプロジェクトにはよくあるファイルで、開発に参加する際のルールが全て記載されています。PR についての決まりは「master ブランチに対して出してね」くらいでした。ここには記載されていませんでしたが、他の開発者はみんなリポジトリをフォークしてから PR を出していたのでそれに習いました。実装する編集したクラスはこの機能です。 Module: display — IPython 8.13.1 documentation Important... ipython.readthedocs.io まあ、方針は定まっていたので実装自体は秒で終わりました。実装する際は以下のような点を意識しました。 IPython は Python3.6 をサポートしているので、:=などの新しい文法を使わない。 破壊的変更をしない。 引数を追加するときはデフォルト値（ex: hoge=None）を持たせて、引数が渡されなくても動作するようにする。 flag=Falseのように新機能はデフォルトではオフにする。 レビューを受ける恥ずかしいですけど百聞は一見に如かずということで、今回の PR のリンクを共有しておきます。 enable autoplay in embed youtube player by yuji96 · Pull Request #13133 · ipython/ipython Added a new argument to autoplay YouTubeVideo in Chrome without muting it.The official Chrome Developers blog on this topic is here:https://developer.chrome.com/blog/autoplay/ github.com 簡単な機能追加だからポチッと承認されて終了かなと思ったけどそんなことはなく、以下のようなコメントを頂きました。 darkerを使ってフォーマットしてください。 変数名をextraからextrasに変更してください。 ドキュメント自動生成時にエラーが起きているので調査する必要があります。 新機能の説明を.rst形式のファイルで書いてください。1. darkerを使ってフォーマットしてください。darker というフォーマッタをインストールしてコマンドを実行するだけで終了しました。一応、コーディング規約には準拠していたのですが、独自の少し細かいルールがあるみたいです。2. 変数名をextraからextrasに変更してください。最初、extrasをextraと読み間違えて「どういうことだ？」と思って反論（？）してしまったのですが、「複数の要素を受け付ける変数名は複数形であるべきだ」という意図のレビューだったそうです。これには同意なのでレビューに従って修正しました。3. ドキュメント自動生成時にエラーが起きているので調査する必要があります。これの解決の方が機能追加よりもむしろ良い貢献だったかもしれません。どうやら同じエラーが複数の PR でも起きていたようで原因調査中だったらしいです。このエラーは、GitHub Actions1 の実行中に起きていたので、その設定ファイル（.github/workflows/docs.yml）を見て、自分の PC で同じコマンドを実行してみました。すると、intersphinx という機能を使って IPython のドキュメントから ipyparallel という別のライブラリのドキュメントを張っている箇所でエラーが起きていました。その原因は単純で、ipyparallel がインベントリと呼ばれる URL の設定を変更したので、IPython もそれに追従する必要があったということです。インベントリは sphinx がインストールされていれば、intersphinx の公式ドキュメントより$ python -msphinx.ext.intersphinx https://ipyparallel.readthedocs.io/en/stable/objects.inv &amp;gt; ipykernelを実行すれば入手できます。（出力が膨大なのでファイルに出力した。）無効になったリンクを修正したらエラーは解決したのでプッシュをしたら、「Thank you very much」「Big thanks」というコメントを頂きテンションが上りました。そして、「この修正部分だけ先に master ブランチに反映させよう」ということになりました。実はこのときの修正箇所はたった 3 ヶ所だったのですが、それだけを 1 つのコミットとしていました。そのおかげでコミットを cherry-pick するだけで簡単にバグ修正単体の PR が作成できました。粒度が細かすぎるのもどうなんだろうと思いましたが今回は成功だったようです。4. 新機能の説明を.rst形式のファイルで書いてください。要は、英作文してくださいってことですね。とりあえず書きました → approve されて merge されました →そしたら、後で作成された PR で 9 割くらい書き直されてました。（笑）メンションされていなかったのでコレを見つけたのは偶然でした。そして完成したのがこちらになります。 7.x Series — IPython 8.13.1 documentation Important... ipython.readthedocs.io 多分、英作文スキルが駄目だったのではなく、単に僕の文章に面白みがなかったことが原因だったみたいです。今回実装した機能が自分が欲しいものだったので自己中っぽさが出ないように、スポットは副産物的な部分に当てて YouTubeVideo はそのついでみたいに書いたのですが、修正された文章だと YouTubeVideo が結構クローズアップされていました。また、サンプルコードは内部の挙動の説明用ではなく実用的なものに置き換わっていて、より魅力的になっていました。これだから自己 PR が苦手なわけだ。（プルリクではない。）感想英語には生まれたころから苦手意識があったので不安でしたが、なんとか会話のキャッチボールは成り立ったようで良かったです。最初は緊張してましたが途中からは楽しくなってました。また、pip install ipythonをして自分が追加した機能が実行されるのは不思議な感覚でした。機械学習系のライブラリを利用してる際によくソースコードを参照して定義を確認するのですが、コードを見てるとそろそろ自分も実装できるレベルになっているのではないかと最近感じてきています。今回、OSS 開発が意外と楽しいということを知ることができたので、日頃から共同開発者という気持ちで利用してアイデアが浮かび次第コントリビュートしたいと思いました。 リモートリポジトリにプッシュしたり、PR を出したときにコマンドを自動で実行してくれるサービス。個人のパブリック・リポジトリなら無料で、このブログでも使っている。 &amp;#8617; " }, { "title": "初めてOSS開発（IPython）にコミットした話 ①", "url": "/posts/first-OSS-1/", "categories": "blog", "tags": "ipython, sphinx, github", "date": "2021-09-27 01:42:00 +0900", "snippet": "ダイジェストJupyterLab で重い処理が終了したら YouTube を流して気付ける様にする機能を作りたい。しかし、Chrome は IFrame の自動再生をデフォルトでは無効化するから、IFrame の属性を編集できる機能を追加したくなり、人生初めて OSS へプルリクエスト（以下、PR）を出すことにした。最終的に承認されて公式ドキュメントにも反映された。 7.x Series — IPython 8.13.1 documentation Important... ipython.readthedocs.io （ただ、自分のための機能は fork してそこで作ればよかったのでは、と後で気づいた。）PR を出すと自分の編集箇所と無関係な箇所で GitHub Workflow がエラーを出したのでついでに直したら、他の PR でも起きてた問題が解消されてらしくお役に立たみたいで嬉しかった。このとき、commit の粒度をちゃんとしていてよかったなと思った。まず、OSS とはOSS とはオープンソースソフトウェアの略で、利用者がソースコードの使用・改変・再配布ができるソフトの総称です。プログラミング言語やその外部ライブラリの多くは OSS です。例えば、こちらの Python の OSS ライブラリ一覧 を見ると Python でデータ分析をしている人にとっては親しみのある名前がたくさん並んでいると思います。OSS の利点は、ソースコードが公開されているので利用者が共同開発者として機能追加・バグ修正をすることができます。開発以外にもバグ報告やドキュメント作成とその翻訳などから貢献することもできます。少し前に接触確認アプリ COCOA のバグが話題になり、その時の朝日新聞に GitHub の issue のスクショが載っていました。（プログラマとしてはよく見る画面が新聞に切り抜きされているのが面白かったです。）あの件も OSS の形をとっていなかったら、一般の方がソースコードを調査することができなかったのでもっと発見が遅れていたことでしょう。コミットしようと思った経緯僕の IPython の使い方今回、私は IPython の開発プロジェクトに参加しました。IPython の「I」は Interactive の略で、ターミナルで$ pythonとしたときに開かれる対話モードをよりリッチにしてくれるライブラリです。Jupyter Lab というライブラリを使うともっとリッチな画面でプログラミングすることができます。対話モードだとターミナルをシャットダウンするまで生成した変数（オブジェクト）が削除されないので、データ分析の様に同じ変数に対して色々な処理を試してみたいときによく使っています。作りたい機能データ分析のアルゴリズムによっては実行終了するまで数十分から 1 時間くらいかかる処理もあったりします。その間に別の作業をしたり PC を放置して寝転んだりして、終わったかなと思ったときに実行画面に戻ってきます。この確認を何回もするのがめんどくさいので IPython 側から人間に知らせる機能を作りたいなと思いました。この手の話題は検索してみると LINE や Slack のように PC のシステム通知を使って（macbook だと）右上にメッセージを表示する。 Slack API を使って bot から DM する。 echo &quot;^G&quot;, say, afplay などの音が出るコマンドを実行する。などいろんなアプローチを見つけました。そこで僕は YouTube を再生する。という方法なら面白いし実装コストも少ないのではないかと思い、開発することにしました。生じた問題 → コントリビュートのきっかけIPython には元々、YouTubeVideo という YouTube を IFrame で埋め込めるクラスが用意されています。1これを使って先ほどの機能を叶えるためには、IFrame で埋め込んだプレーヤーを自動再生することが必須なのですが、YouTube 側の過去の仕様変更により Chrome では allow=&quot;autoplay&quot; を IFrame の属性に追加しなければ自動再生できません。しかし、IPython で提供されている YouTubeVideo からは IFrame の属性をいじることができませんでした。このとき、「属性を追加するインターフェースは IPython 側が用意してくれてもいいのになー、実装も簡単だし。」と感じました。そこで、OSS の利用者は同時に共同開発者であるということを思い出し、以前から OSS 開発に興味があったことから、PR を出してみることにしました。2ここから実際にやったことの話になり流れが変わるので一旦終わりにします。初めて OSS 開発（IPython）にコミットした話（その２）に続く。 コミットログを見ると 10 年以上前からあるみたい。 &amp;#8617; 「別に IFrame を埋め込む機能を 0 から作ればよくね？」だって？もちろんそれは僕も思いました。PR 出した後にね！！（ときすでにお寿司） &amp;#8617; " } ]
